<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-*Private/*my-Projects" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">*my-Projects | Irtebat&#x27;s Second Brain</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://irtebat.github.io/*Private/*my-Projects"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="*my-Projects | Irtebat&#x27;s Second Brain"><meta data-rh="true" name="description" content="When I joined Confluent in 2022, I worked alongside the Connect and Confluent Platform engineering groups, on tuning open-source connectors and building services that supported migrations. As of 2025, I am part of a specialised engineering team in APAC that builds services used by some of our largest customers. The role has both a technical and a strategic aspect."><meta data-rh="true" property="og:description" content="When I joined Confluent in 2022, I worked alongside the Connect and Confluent Platform engineering groups, on tuning open-source connectors and building services that supported migrations. As of 2025, I am part of a specialised engineering team in APAC that builds services used by some of our largest customers. The role has both a technical and a strategic aspect."><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://irtebat.github.io/*Private/*my-Projects"><link data-rh="true" rel="alternate" href="https://irtebat.github.io/*Private/*my-Projects" hreflang="en"><link data-rh="true" rel="alternate" href="https://irtebat.github.io/*Private/*my-Projects" hreflang="x-default"><link rel="stylesheet" href="/assets/css/styles.d0226fa2.css">
<script src="/assets/js/runtime~main.ab72de78.js" defer="defer"></script>
<script src="/assets/js/main.a62294d4.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>document.documentElement.setAttribute("data-theme","light"),document.documentElement.setAttribute("data-theme-choice","light"),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><b class="navbar__title text--truncate"></b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/">Home</a><a class="navbar__item navbar__link" href="/about">About Me</a><a class="navbar__item navbar__link" href="/concepts">Concepts</a><a class="navbar__item navbar__link" href="/playbooks">Playbooks</a><a class="navbar__item navbar__link" href="/system-design">System Design</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><div class="navbarSearchContainer_Bca1"><div class="navbar__search searchBarContainer_NW3z" dir="ltr"><input placeholder="Search" aria-label="Search" class="navbar__search-input searchInput_YFbd" value=""><div class="loadingRing_RJI3 searchBarLoadingRing_YnHq"><div></div><div></div><div></div><div></div></div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><main class="docMainContainer_TBSr docMainContainerEnhanced_lQrH"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>*my-Projects</h1></header><p>When I joined Confluent in 2022, I worked alongside the Connect and Confluent Platform engineering groups, on tuning open-source connectors and building services that supported migrations. As of 2025, I am part of a specialised engineering team in APAC that builds <strong>services</strong> used by some of our largest customers. The role has both a technical and a strategic aspect.</p>
<p>The context is, for Confluent, a select number of large enterprises have advanced requirements that go beyond the standard product capabilities. These select customers ( telcos, banks ) often face challenges that require <strong>dedicated engineering focus.</strong> Our team works with these select customers ( mostly banks, telcos ) to understand their operational challenges, and then <strong>design and build accelerator services</strong> that make enterprise adoption faster and easier.</p>
<p>These accelerators typically fall into two categories:</p>
<ol>
<li class="">
<p><strong>Product Gap Extensions</strong> – solutions that fill functional or operational gaps until they are natively supported in the product.</p>
</li>
<li class="">
<p><strong>Platform Deployment Accelerators</strong> – frameworks that simplify and standardize the deployment of data platforms across enterprise environments for : enabling an automated pipeline setup and monitoring</p>
</li>
</ol>
<p>In both cases, our focus is on engineering solutions that are <strong>scalable, and reusable.</strong></p>
<p>The intent is to <strong>unblock enterprises</strong> through engineering, while feeding back learnings that can inform product evolution.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="example-of-product-gap-extensions"><strong>Example of Product Gap Extensions:</strong><a href="#example-of-product-gap-extensions" class="hash-link" aria-label="Direct link to example-of-product-gap-extensions" title="Direct link to example-of-product-gap-extensions" translate="no">​</a></h3>
<p>==<strong>CDC-aware orchestration service</strong>==</p>
<p><strong>Situation:</strong></p>
<p>Our account team was pursuing enterprises to use <strong>Kafka as a low-cost disaster recovery solution</strong> for relational databases by leveraging <strong>Kafka’s Connector ecosystem</strong>.</p>
<p>For these replication setups, a key operational challenge surfaced at the ideation stage: when <strong>DDL changes would be applied directly on databases</strong> under replication through <strong>Kafka Connect</strong>, they would <strong>break replication</strong> or cause <strong>schema drift</strong> between source and target systems.</p>
<p>We needed a scalable design of a kafka-enabled DR solution, and a <strong>safe, automated way</strong> to apply DDLs while maintaining replication consistency across environments.</p>
<p><strong>Task:</strong></p>
<p>Our team was brought in to <strong>design the overall Kafka-as-DR replication solution</strong> and most importantly develop a solution to address this ddl-related reliability challenge.</p>
<p>I broke down the ask into two broad problem statements :</p>
<ol>
<li class="">
<p><strong>Design a scalable CDC ingestion and replication platform</strong>, and</p>
</li>
<li class="">
<p><strong>Build a CDC-aware orchestration service</strong> to coordinate DDL application with ongoing replication — effectively making schema changes <strong>transactional with respect to CDC pipelines</strong>.</p>
</li>
</ol>
<p>I want to talk about the cdc-aware orchestration service. I can cover the DR-platform design afterwards.</p>
<p><strong>Action:</strong></p>
<p>I developed a <strong>FastAPI microservice</strong> that serves as the control plane for DDL operations. It has 3 embedded services:</p>
<ul>
<li class="">
<p><strong>A Kafka Connect client</strong> that uses REST APIs to reconfigure, drain, pause, or resume connectors.</p>
</li>
<li class="">
<p>A Kakfa Client that used Confluent-kafka python SDK to track and modify kafka flags ( connect offsets ).</p>
</li>
<li class="">
<p>Database Client that using JDBC API to apply the DDLs in a controlled manner. ( JayDeBeAPI — JPype Java Bridge )</p>
</li>
</ul>
<p>The workflow looked like this:</p>
<ol>
<li class="">
<p>A DDL request was received by the service.</p>
</li>
<li class="">
<p>The statement was <strong>classified</strong> (create, alter, drop, index, view) — parsed using sqlparse</p>
</li>
<li class="">
<p>Based on classification, the service picks a strategy, orchestrates the replication pipeline as per the applicable strategy.</p>
</li>
<li class="">
<p>For audit, every operation was recorded in a <strong>Kafka-backed metadata log</strong> that tracks execution state.</p>
</li>
<li class="">
<p>It enforced <strong>selective concurrency</strong> — serial execution for structural DDLs (<code>ALTER TABLE</code>, <code>DROP COLUMN</code>), parallel for non-structural ones (views, indexes).</p>
<ul>
<li class="">
<p>For synchronous calls : await asyncio.gather(run_in_threadpool ( blocking_fn1)…)</p>
</li>
<li class="">
<p>(asyncio.get_running_loop()).run_in_executor (()→getProcessPoolExecutor(),fn, **params)</p>
</li>
</ul>
</li>
</ol>
<p>The service was packaged as a <strong>image for secure bastion deployment</strong>, providing controlled access between application and data layers.</p>
<p><strong>Result:</strong></p>
<p>The solution eliminated replication breakages due to unsynchronized DDLs and became the <strong>standard interface</strong> we recommended for SQL teams to use to manage schema changes safely for the proposed kafka-as-a-DR solution.</p>
<p>The service was later <strong>shared with Confluent’s account and professional services teams</strong> to be used across enterprise engagements.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="example-of-platform-deployment-accelerators"><strong>Example of Platform Deployment Accelerators:</strong><a href="#example-of-platform-deployment-accelerators" class="hash-link" aria-label="Direct link to example-of-platform-deployment-accelerators" title="Direct link to example-of-platform-deployment-accelerators" translate="no">​</a></h3>
<p>==<strong>Flink-based Hybrid Analytics Framework</strong>==</p>
<p><strong>Context:</strong></p>
<p>Across multiple large enterprise implementations, we noticed a recurring pattern — teams needed to <strong>combine batch data from systems like SAP HANA or ERP exports</strong> with <strong>real-time event streams</strong> to maintain consistent operational views, such as inventory reconciliation or order fulfillment.</p>
<p>Each engagement rebuilt this logic from scratch — deploying Flink Session Clusters, building custom Flink images with application logic.</p>
<p><strong>Task:</strong></p>
<p>I was asked to <strong>design and implement a reusable, production-grade analytics framework</strong> that could standardize how enterprise teams build these <strong>stream processing pipelines.</strong></p>
<p><strong>Action:</strong></p>
<p>I engineered a <strong>Flink-based analytics framework</strong> deployed as a <strong>Kubernetes-native service</strong>, with <strong>Kafka as the data backbone</strong> and a declarative configuration model to define data pipelines.</p>
<p>The framework comprised three core layers:</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="1-topology-definition-layer">1. <strong>Topology Definition Layer</strong><a href="#1-topology-definition-layer" class="hash-link" aria-label="Direct link to 1-topology-definition-layer" title="Direct link to 1-topology-definition-layer" translate="no">​</a></h3>
<ul>
<li class="">
<p>Users define <strong>sources, transformations, and sinks</strong> in a configuration YAML.</p>
</li>
<li class="">
<p>Each source specifies its <strong>semantics</strong> (append or upsert), <strong>primary key</strong>, and <strong>watermarking strategy</strong>.</p>
</li>
<li class="">
<p>Transformation logic and <strong>joins</strong> are declared with parameters such as join type, keys, and TTL (for state expiration).</p>
</li>
<li class="">
<p>Under the hood, the library generates <strong>Table API pipelines</strong> dynamically at runtime</p>
</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="2-deployment--governance-layer">2. <strong>Deployment &amp; Governance Layer</strong><a href="#2-deployment--governance-layer" class="hash-link" aria-label="Direct link to 2-deployment--governance-layer" title="Direct link to 2-deployment--governance-layer" translate="no">​</a></h3>
<ul>
<li class="">
<p>A <strong>Helm-based deployment blueprint</strong> standardized runtime configuration across environments.</p>
</li>
<li class="">
<p>Integrated with <strong>Schema Registry</strong> for contract governance</p>
</li>
<li class="">
<p>Included <strong>state checkpointing</strong>, <strong>exactly-once guarantees</strong>, and <strong>error recovery</strong> patterns as defaults.</p>
</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="3-observability--slo-enforcement">3. <strong>Observability &amp; SLO Enforcement</strong><a href="#3-observability--slo-enforcement" class="hash-link" aria-label="Direct link to 3-observability--slo-enforcement" title="Direct link to 3-observability--slo-enforcement" translate="no">​</a></h3>
<ul>
<li class="">
<p>Exposed <strong>Prometheus metrics</strong>.</p>
</li>
<li class="">
<p>Packaged <strong>Grafana dashboards</strong> for SLO visualization and alerting.</p>
</li>
<li class="">
<p>Defined baseline SLOs for <strong>latency</strong>, <strong>throughput</strong>, and <strong>freshness</strong>, enforcing them through the framework’s health monitors.</p>
</li>
</ul>
<p>Everything was <strong>containerized and versioned</strong>, enabling consistent reuse across multiple engagements rather than rebuilding per customer.</p>
<hr>
<p><strong>Result:</strong></p>
<p>The framework became a <strong>reference architecture within Confluent</strong> for real-time reconciliation and analytics use cases. It:</p>
<ul>
<li class="">
<p>Reduced setup and implementation time for new customer programs.</p>
</li>
<li class="">
<p>Improved <strong>consistency</strong>, <strong>observability</strong>, and <strong>maintainability</strong> of hybrid Flink jobs.</p>
</li>
<li class="">
<p>Enabled a <strong>productized delivery model</strong> — turning what used to be per-customer custom work into an <strong>internal, versioned software component</strong>.</p>
</li>
</ul>
<p>The key engineering takeaway was that <strong>streaming analytics should be treated like a product</strong> — with standardized APIs, observability, and lifecycle management — rather than one-off data pipelines. This shift allowed Confluent’s teams to scale delivery quality and reliability across enterprise environments.</p>
<hr>
<p><strong>Self-optimizing CDC ingestion and replication platform</strong></p>
<p>using Kafka Connect and custom Java sink modules for Oracle, Postgres, and SQL Server.</p>
<p>• Built a <strong>Watchdog Service</strong> that periodically analyzed connector performance and DB performance metrics and <strong>auto-tuned CDC source and JDBC sink parameters</strong> to sustain required throughput under varying loads.</p>
<p>• Developed a <strong>Streaming Lag Tracker service</strong> for real-time monitoring of source-to-sink latency and anomaly detection.</p>
<p>• Containerized the full stack (Connect, Watchdog, Lag Tracker) with Helm-based deployment assets for Kubernetes, including Prometheus/Grafana observability and DR automation.</p>
<p>• Reduced new-domain onboarding effort by over 60% and stabilized cross-database CDC throughput within SLA targets.</p>
<p><strong>Situation:</strong></p>
<p>Several enterprise teams were modernizing data pipelines to Confluent Kafka, but each business domain was spending weeks building one-off integrations for different source and target databases. The goal was to industrialize this — a reusable, deployable, extendable and observable platform for CDC-based data movement.</p>
<p>I used Kafka Connect as the backbone.</p>
<ul>
<li class="">
<p>For sources, we leveraged Oracle and Postgres CDC connectors.</p>
</li>
<li class="">
<p>For sinks, I built a <strong>custom JDBC application</strong> running inside the Connect runtime to handle upsert semantics based on record metadata.</p>
</li>
<li class="">
<p>Both components were packaged as images with Kubernetes deployment templates and observability baked in.</p>
</li>
</ul>
<p><strong>Key Software Piece — Watchdog Service:</strong></p>
<p>The major service that facilitated the pipeline was a <strong>Watchdog Service for Day-2 operations ( a smart pipeline observability tool )</strong> — a feedback controller that continuously reads metrics from Prometheus and Kafka Connect REST endpoints.</p>
<p>It periodically adjusted connector parameters — like batch size, commit interval, and fetch delay — to maintain optimal lag and throughput. This made the system adaptive to workload spikes or network fluctuations. It also implemented restart and recovery logic for failed tasks.</p>
<p><strong>Streaming Lag Tracker:</strong></p>
<p>Alongside that, I built a <strong>streaming service</strong> that computed and visualized lag from source timestamp to sink acknowledgment in near real time. It powered proactive alerting and automated SLA breach detection.</p>
<p><strong>Ops and Reliability:</strong></p>
<p>All components were deployed on Kubernetes with Helm.</p>
<p>We integrated Grafana dashboards, implemented cross-region DR with mirrored offsets, and built CI/CD pipelines for deterministic rollouts.”</p>
<p><strong>Impact:</strong></p>
<p>This platform turned what used to be weeks of per-domain setup into a few hours of configuration. Infra teams measured more than 60% faster onboarding, and operational metrics stabilized around single-digit-second average lag for steady loads.”</p>
<hr>
<p><strong>Schema Reconciliation Framework</strong></p>
<p>Built a Java-based Schema Reconciliation Framework enabling active–active Schema Registry synchronization, a capability not natively supported in Confluent’s platform. Designed algorithms to detect, validate, and reconcile schema deltas across registries while preserving compatibility and semantic integrity. Delivered controlled rollout and rollback pipelines with audit logging and GitOps integration, standardizing schema governance across multi-region environments.</p>
<hr>
<p><strong>Situation:</strong></p>
<p>Confluent’s Schema Registry provides strong governance within a single environment, but it does not natively support <em>active–active multi-region synchronization</em>.</p>
<p>In large environments, teams often maintain parallel registries — for example, cross-cloud or region-isolated setups — and need to keep them in sync safely.</p>
<p><strong>Task:</strong></p>
<p>I built a <strong>Schema Reconciliation Framework</strong> that could identify and safely propagate schema deltas between registries, ensuring compatibility and correctness across regions.</p>
<p><strong>Action:</strong></p>
<p>I designed a <strong>Java-based service</strong> that periodically or on-demand:</p>
<ul>
<li class="">
<p>Discovers all subjects and schema versions from multiple registries via their REST APIs.</p>
</li>
<li class="">
<p>Computes a <strong>semantic delta plan</strong> by normalizing and diffing schemas.</p>
</li>
<li class="">
<p>Validates compatibility modes and field-level changes before propagation.</p>
</li>
<li class="">
<p>Executes <strong>controlled rollouts</strong> to secondary registries, maintaining transactional logs and rollback checkpoints.</p>
<p>It also provides a REST interface and integrates with GitOps pipelines so schema promotion can be done as part of CI/CD.</p>
<p>The framework was packaged as a <strong>lightweight container service</strong> deployed on Kubernetes — stateless, idempotent, and fully auditable.</p>
</li>
</ul>
<p><strong>Result:</strong></p>
<p>This closed a major operational gap for active–active schema governance.</p>
<p>It allowed enterprises to synchronize Schema Registries safely across environments, eliminated manual drift, and introduced formal schema lifecycle management where none existed natively in the platform.”</p></div></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#example-of-product-gap-extensions" class="table-of-contents__link toc-highlight"><strong>Example of Product Gap Extensions:</strong></a></li><li><a href="#example-of-platform-deployment-accelerators" class="table-of-contents__link toc-highlight"><strong>Example of Platform Deployment Accelerators:</strong></a></li><li><a href="#1-topology-definition-layer" class="table-of-contents__link toc-highlight">1. <strong>Topology Definition Layer</strong></a></li><li><a href="#2-deployment--governance-layer" class="table-of-contents__link toc-highlight">2. <strong>Deployment &amp; Governance Layer</strong></a></li><li><a href="#3-observability--slo-enforcement" class="table-of-contents__link toc-highlight">3. <strong>Observability &amp; SLO Enforcement</strong></a></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2026 Irtebat's Brain Comics • Printed in the Cloud.</div></div></div></footer></div>
</body>
</html>